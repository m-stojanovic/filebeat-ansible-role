###################### Filebeat Configuration Example #########################

# This file is an example configuration file highlighting only the most common
# options. The filebeat.reference.yml file from the same directory contains all the
# supported options with more comments. You can use it as a reference.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/filebeat/index.html

# For more available modules and options, please see the filebeat.reference.yml sample
# configuration file.

# ============================== Filebeat inputs ===============================

filebeat.inputs:

{% if nginx is defined %}
- type: log
  enabled: true
  paths:
    - /var/log/nginx/access.log
  fields:
    kafka_topic: nginx-access-logs

- type: log
  enabled: true
  paths: 
    - /var/log/nginx/error.log
  fields:
    kafka_topic: nginx-error-logs
{% endif %}

{% if antmedia is defined %}
- type: log
  enabled: true
  paths:
    - /usr/local/antmedia/log/ant-media-server.log
  fields:
    kafka_topic: ant-media-server-logs

- type: log
  enabled: true
  paths:
    - /usr/local/antmedia/log/antmedia-error.log
  fields:
    kafka_topic: ant-media-error-logs
{% endif %}

{% if nexus is defined %}
- type: log
  enabled: true
  paths:
    - /opt/sonatype-work/nexus3/log/audit/audit.log
  fields:
    kafka_topic: nexus-audit-logs

- type: log
  enabled: true
  paths:
    - /opt/sonatype-work/nexus3/log/request.log
  fields:
    kafka_topic: nexus-request-logs

- type: log
  enabled: true
  paths:
    - /opt/sonatype-work/nexus3/log/tasks/repository.cleanup-2020*.log
  fields:
    kafka_topic: nexus-cleanup-logs
{% endif %}

{% if jenkins is defined %}
- type: log
  enabled: true
  paths:
    - /var/log/jenkins/jenkins.log
  fields:
    kafka_topic: jenkins-master-logs
{% endif %}

{% if debian is defined %}
- type: log
  enabled: true
  paths:
    - /var/log/syslog
  fields:
    kafka_topic: debian-sys-logs
  exclude_lines: ['#011INFO#','#011WARN#','filebeat']

- type: log
  enabled: true
  paths:
    - /var/log/auth.log
  fields:
    kafka_topic: debian-auth-logs
{% endif %}

{% if redhat is defined %}
- type: log
  enabled: true
  paths: 
    - /var/log/messages
  fields:
    kafka_topic: redhat-message-logs
  exclude_lines: ['#011INFO#','#011WARN#','filebeat']

- type: log
  enabled: true
  paths: 
    - /var/log/secure
  fields:
    kafka_topic: redhat-secure-logs
{% endif %}

{% if services is defined %}
- type: log
  enabled: true
  paths:
    - /services/*/logfile
  fields:
    kafka_topic: service-logs
{% endif %}

{% if hostvar is defined %}
# Account Belgrade
- type: s3
  queue_url: {{ sqs_queue_url_belgrade }}
  visibility_timeout: 300s
  access_key_id: "{{ aws_access_key_belgrade_acc }}"
  secret_access_key: "{{ aws_secret_key_belgrade_acc }}"
  aws_region: ap-south-1
  credential_profile_name: filebeat
  fields:
    kafka_topic: s3-belgrade-cloudtrail-logs

# Account Dev
- type: s3
  queue_url: {{ sqs_queue_url_dev }}
  visibility_timeout: 300s
  access_key_id: "{{ aws_access_key_dev_acc }}"
  secret_access_key: "{{ aws_secret_key_dev_acc }}"
  aws_region: eu-west-1
  credential_profile_name: filebeat
  fields:
    kafka_topic: s3-dev-cloudtrail-logs

# Account Prod01 
- type: s3
  queue_url: {{ sqs_queue_url_prod01 }}
  visibility_timeout: 300s
  access_key_id: "{{ aws_access_key_prod01_acc }}"
  secret_access_key: "{{ aws_secret_key_prod01_acc }}"
  aws_region: us-east-1
  credential_profile_name: filebeat
  fields:
    kafka_topic: s3-prod01-cloudtrail-logs

# Account Prod02
- type: s3
  queue_url: {{ sqs_queue_url_prod02 }}
  visibility_timeout: 300s
  access_key_id: "{{ aws_access_key_prod02_acc }}"
  secret_access_key: "{{ aws_secret_key_prod02_acc }}"
  aws_region: us-east-1
  credential_profile_name: filebeat
  fields:
    kafka_topic: s3-prod02-cloudtrail-logs
{% endif %}

# ============================== Filebeat modules ==============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: false

  # Period on which files under path should be checked for changes
  #reload.period: 10s

# ================================= Dashboards =================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here or by using the `setup` command.
#setup.dashboards.enabled: false

# The URL from where to download the dashboards archive. By default this URL
# has a value which is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

# =================================== Kibana ===================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:

  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  #host: "localhost:5601"

  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:

# ---------------------------- Elasticsearch Output ----------------------------
#output.elasticsearch:
  # Array of hosts to connect to.
  #hosts: ["localhost:9200"]

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"

# ------------------------------ Logstash Output -------------------------------
#output.logstash:
  # The Logstash hosts
  # hosts: ["localhost:5000"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  # sl.key: "/etc/pki/client/cert.key"

# ------------------------------ Kafka Output -------------------------------
output.kafka:
  codec.format:
    string: '%{[host.name]} %{[message]}'
  hosts: ["kafka-broker-1:9092", "kafka-broker-2:9093", "kafka-broker-3:9094"]
  topic: '%{[fields.kafka_topic]}'
  ssl.enabled: true
  ssl.certificate_authorities: ["/home/{{ user }}/ssl/snakeoil-ca-1.crt"]
  ssl.key: "/home/{{ user }}/ssl/snakeoil-ca-1.pem"
  ssl.certificate: "/home/{{ user }}/ssl/snakeoil-ca-1-crt.pem"
  partition.round_robin:
    reachable_only: false
  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000
